%YAML 1.2
---
# Dataset preprocessing and augmentation YAML configuration file
augmentations_recipes:
  - basic_augmentation: &basic_augmentation
    keep_same_input_shape: true # Whether augmented images will be croped their respective initial input image sizes or not
    random_transform_order: true
    random_transform_dropout: 0.5 # Probability that each augmentation tranforms will be droped/ignored for a given image augmentation
    transforms:
      - crop: false
      - brightness: 0.2 # Random brightness variance (assumes pixel data is normalized)
      - contrast: 0.1 # Contrast transform variance
      - tweak_colors: 0.1
      - gamma: 0.05 # gamma tweaking variance
      - noise: 0.1
      - rotate: [-0.4, 0.4] # e.g., `[-|a|, |b|]` rotation range means random rotation will be sampled from a gaussian distribution truncated between -180x|a|° and 180x|b|°, with gaussian variance beeing proportional to `|a|-|b|`
      - translate: 0.2 # variance of image translation transform
      - scale: 0.2
      - smooth_non_linear_deformation: false # Strenght of a non-linear image deformation (set to null, false or 0 to disable)
  - singan_augmentation: &singan_augmentation
    <<: *basic_augmentation
    transforms_additional:
      - distilled_singan_augmentation: true

basic_preprocessing_procedure: &basic_preprocessing_procedure
  validset_ratio: 0.2
  testset_ratio: 0.2
  cache: true
  preprocessing:
    - pil_to_tensor
    - normalize
  augmentations: [*basic_augmentation]

imagenet_stats: ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
cifar_stats: ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261])
mnist_stats : ([0.15, 0.15, 0.15], [0.15, 0.15, 0.15])

cifar10_preprocessing: &cifar10_preprocessing
  <<: *basic_preprocessing_procedure

cifar100_preprocessing:
  <<: *cifar10_preprocessing

imagenet32_prerocessing:
  <<: *basic_preprocessing_procedure



...
%YAML 1.2
%TAG !py! tag:yaml.org,2002:python/object:  
---

# Hyper-parameter YAML configuration file
basic_training_params: &basic_ignite_training
  scheduler:
    type: !py!ignite.contrib.handlers.PiecewiseLinear
    eval_args: ["milestones_values"]
    kwargs:
      milestones_values: "[[0, 0.0], [20 * len(trainset), hp['lr']], [hp['epochs'] * len(trainset), 0.0]]"
  # lr_scheduler:
  #   type: !py!deepcv.meta.one_cycle.OneCyclePolicy
  #   kwargs:
  #     base_lr: 1e-4
  #     max_lr: 0.1
  #     base_momentum: 1e-4
  #     max_momentum: 1e-2
  resume_from: ""
  crash_iteration: -1
  validate_every: 1 # Epoch
  checkpoint_every: # Iters
  log_model_grads_every: # Iters
  display_iters: # Iters
  seed: 563454
  deterministic: false
  dist_url: # "env://"
  dist_backend: "" # Set, for example, nccl (torch.distributed.Backend.NCCL) for distributed training using nccl backend

object_detector:
  <<: *basic_ignite_training
  output_path: ""
  epochs: 80
  batch_size: 32
  optimizer_opts: { lr: 0.1, momentum: 0., weight_decay: 0., nesterov: true }
  model:
    act_fn: !py!deepcv.torch.nn.ReLU # evaluated at runtime in python code
    dropout_prob: 0.0
    batch_norm: { affine: true, eps: !!float 1e-05, momentum: 0.07359778246238029 }
    architecture:
      - "conv2d": &input1 { kernel_size: [3, 3], out_channels: 4, padding: 0 }
      - "conv2d": { kernel_size: [3, 3], out_channels: 4, padding: 0 }
      - "conv2d": { kernel_size: [3, 3], out_channels: 4, padding: 0 }
      - "avg_pooling": &reidual1 { kernel_size: [2, 2], stride: [2, 2] }
      - "conv2d": { kernel_size: [5, 5], out_channels: 16, padding: 0 }
      - "conv2d": { kernel_size: [5, 5], out_channels: 16, padding: 0 }
      - "avg_pooling": { kernel_size: [2, 2], stride: [2, 2] }
      - "append": "residual1"
      - "conv2d": { kernel_size: [5, 5], out_channels: 32, padding: 2 }
      - "conv2d": { kernel_size: [7, 7], out_channels: 32, padding: 3 }
      - "avg_pooling": { kernel_size: [2, 2], stride: [2, 2] }
      - "conv2d": { kernel_size: [5, 5], out_channels: 64, padding: 2 }
      - !py!deepcv.meta.nn.Flatten: null
      - "fully_connected": &output1 null

# object_detector_hp_search:
#   cross_validation: false
#   hyperparams:
#     - optimizer_opts.lr: linear([1e-6, 5e-3])
#     - optimizer.momentum: log_linear([1e-8, 5e-3])
#     - optimizer.weight_decay: log_linear([1e-10, 5e-4])
#     - choice:
#       - model.dropout_prob: choice([0., linear([0.1, 0.6])])
#       - model.batch_norm: choice({ affine: true, eps: !!float 1e-05, momentum: 0.07359778246238029 }, null)